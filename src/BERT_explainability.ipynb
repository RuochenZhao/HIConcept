{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hila-chefer/Transformer-Explainability/blob/main/BERT_explainability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4-XGl_Zw6Aht"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from BERT_explainability.modules.BERT.ExplanationGenerator import Generator\n",
    "from BERT_explainability.modules.BERT.BertForSequenceClassification import BertForSequenceClassification\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertConfig\n",
    "from BERT_explainability.modules.BERT.ExplanationGenerator import Generator\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "from captum.attr import (\n",
    "    visualization\n",
    ")\n",
    "import torch\n",
    "from conceptshap import *\n",
    "import numpy as np\n",
    "from BERT_explainability.modules.layers_ours import *\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "import os\n",
    "from BERT_explainability.modules.layers_ours import NormLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VakYjrkC6C3S"
   },
   "outputs": [],
   "source": [
    "# # model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\").to(\"cuda\")\n",
    "# save_dir = 'models/imdb/bert/imdb_weights'\n",
    "# # model = DistilBertForSequenceClassification.from_pretrained(save_dir)\n",
    "# model = BertForSequenceClassification.from_pretrained(save_dir).to(device)\n",
    "# model.eval()\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(1, 512, 768)\n",
    "t.flatten(start_dim = 0, end_dim = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.children())\n",
    "# bertmodel, dropout, linear\n",
    "\n",
    "model.classifier\n",
    "# linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (matmul1): MatMul()\n",
       "       (matmul2): MatMul()\n",
       "       (softmax): Softmax(dim=-1)\n",
       "       (add): Add()\n",
       "       (mul): Mul()\n",
       "       (clone): Clone()\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (add): Add()\n",
       "     )\n",
       "     (clone): Clone()\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELU()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (add): Add()\n",
       "   )\n",
       "   (clone): Clone()\n",
       " ),\n",
       " BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (matmul1): MatMul()\n",
       "       (matmul2): MatMul()\n",
       "       (softmax): Softmax(dim=-1)\n",
       "       (add): Add()\n",
       "       (mul): Mul()\n",
       "       (clone): Clone()\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (add): Add()\n",
       "     )\n",
       "     (clone): Clone()\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELU()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (add): Add()\n",
       "   )\n",
       "   (clone): Clone()\n",
       " ),\n",
       " BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (matmul1): MatMul()\n",
       "       (matmul2): MatMul()\n",
       "       (softmax): Softmax(dim=-1)\n",
       "       (add): Add()\n",
       "       (mul): Mul()\n",
       "       (clone): Clone()\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (add): Add()\n",
       "     )\n",
       "     (clone): Clone()\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELU()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (add): Add()\n",
       "   )\n",
       "   (clone): Clone()\n",
       " ),\n",
       " BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (matmul1): MatMul()\n",
       "       (matmul2): MatMul()\n",
       "       (softmax): Softmax(dim=-1)\n",
       "       (add): Add()\n",
       "       (mul): Mul()\n",
       "       (clone): Clone()\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (add): Add()\n",
       "     )\n",
       "     (clone): Clone()\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELU()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (add): Add()\n",
       "   )\n",
       "   (clone): Clone()\n",
       " ),\n",
       " BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (matmul1): MatMul()\n",
       "       (matmul2): MatMul()\n",
       "       (softmax): Softmax(dim=-1)\n",
       "       (add): Add()\n",
       "       (mul): Mul()\n",
       "       (clone): Clone()\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (add): Add()\n",
       "     )\n",
       "     (clone): Clone()\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELU()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (add): Add()\n",
       "   )\n",
       "   (clone): Clone()\n",
       " ),\n",
       " BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (matmul1): MatMul()\n",
       "       (matmul2): MatMul()\n",
       "       (softmax): Softmax(dim=-1)\n",
       "       (add): Add()\n",
       "       (mul): Mul()\n",
       "       (clone): Clone()\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (add): Add()\n",
       "     )\n",
       "     (clone): Clone()\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELU()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (add): Add()\n",
       "   )\n",
       "   (clone): Clone()\n",
       " ),\n",
       " BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (matmul1): MatMul()\n",
       "       (matmul2): MatMul()\n",
       "       (softmax): Softmax(dim=-1)\n",
       "       (add): Add()\n",
       "       (mul): Mul()\n",
       "       (clone): Clone()\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (add): Add()\n",
       "     )\n",
       "     (clone): Clone()\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELU()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (add): Add()\n",
       "   )\n",
       "   (clone): Clone()\n",
       " ),\n",
       " BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (matmul1): MatMul()\n",
       "       (matmul2): MatMul()\n",
       "       (softmax): Softmax(dim=-1)\n",
       "       (add): Add()\n",
       "       (mul): Mul()\n",
       "       (clone): Clone()\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (add): Add()\n",
       "     )\n",
       "     (clone): Clone()\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELU()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (add): Add()\n",
       "   )\n",
       "   (clone): Clone()\n",
       " ),\n",
       " BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (matmul1): MatMul()\n",
       "       (matmul2): MatMul()\n",
       "       (softmax): Softmax(dim=-1)\n",
       "       (add): Add()\n",
       "       (mul): Mul()\n",
       "       (clone): Clone()\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (add): Add()\n",
       "     )\n",
       "     (clone): Clone()\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELU()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (add): Add()\n",
       "   )\n",
       "   (clone): Clone()\n",
       " ),\n",
       " BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (matmul1): MatMul()\n",
       "       (matmul2): MatMul()\n",
       "       (softmax): Softmax(dim=-1)\n",
       "       (add): Add()\n",
       "       (mul): Mul()\n",
       "       (clone): Clone()\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (add): Add()\n",
       "     )\n",
       "     (clone): Clone()\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELU()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (add): Add()\n",
       "   )\n",
       "   (clone): Clone()\n",
       " ),\n",
       " BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (matmul1): MatMul()\n",
       "       (matmul2): MatMul()\n",
       "       (softmax): Softmax(dim=-1)\n",
       "       (add): Add()\n",
       "       (mul): Mul()\n",
       "       (clone): Clone()\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (add): Add()\n",
       "     )\n",
       "     (clone): Clone()\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELU()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (add): Add()\n",
       "   )\n",
       "   (clone): Clone()\n",
       " ),\n",
       " BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (matmul1): MatMul()\n",
       "       (matmul2): MatMul()\n",
       "       (softmax): Softmax(dim=-1)\n",
       "       (add): Add()\n",
       "       (mul): Mul()\n",
       "       (clone): Clone()\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (add): Add()\n",
       "     )\n",
       "     (clone): Clone()\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELU()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (add): Add()\n",
       "   )\n",
       "   (clone): Clone()\n",
       " )]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.bert.encoder.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.bert.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout(p=0.1, inplace=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.children())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (matmul1): MatMul()\n",
       "        (matmul2): MatMul()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (add): Add()\n",
       "        (mul): Mul()\n",
       "        (clone): Clone()\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (add): Add()\n",
       "      )\n",
       "      (clone): Clone()\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELU()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (add): Add()\n",
       "    )\n",
       "    (clone): Clone()\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (matmul1): MatMul()\n",
       "        (matmul2): MatMul()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (add): Add()\n",
       "        (mul): Mul()\n",
       "        (clone): Clone()\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (add): Add()\n",
       "      )\n",
       "      (clone): Clone()\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELU()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (add): Add()\n",
       "    )\n",
       "    (clone): Clone()\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (matmul1): MatMul()\n",
       "        (matmul2): MatMul()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (add): Add()\n",
       "        (mul): Mul()\n",
       "        (clone): Clone()\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (add): Add()\n",
       "      )\n",
       "      (clone): Clone()\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELU()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (add): Add()\n",
       "    )\n",
       "    (clone): Clone()\n",
       "  )\n",
       "  (3): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (matmul1): MatMul()\n",
       "        (matmul2): MatMul()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (add): Add()\n",
       "        (mul): Mul()\n",
       "        (clone): Clone()\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (add): Add()\n",
       "      )\n",
       "      (clone): Clone()\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELU()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (add): Add()\n",
       "    )\n",
       "    (clone): Clone()\n",
       "  )\n",
       "  (4): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (matmul1): MatMul()\n",
       "        (matmul2): MatMul()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (add): Add()\n",
       "        (mul): Mul()\n",
       "        (clone): Clone()\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (add): Add()\n",
       "      )\n",
       "      (clone): Clone()\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELU()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (add): Add()\n",
       "    )\n",
       "    (clone): Clone()\n",
       "  )\n",
       "  (5): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (matmul1): MatMul()\n",
       "        (matmul2): MatMul()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (add): Add()\n",
       "        (mul): Mul()\n",
       "        (clone): Clone()\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (add): Add()\n",
       "      )\n",
       "      (clone): Clone()\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELU()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (add): Add()\n",
       "    )\n",
       "    (clone): Clone()\n",
       "  )\n",
       "  (6): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (matmul1): MatMul()\n",
       "        (matmul2): MatMul()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (add): Add()\n",
       "        (mul): Mul()\n",
       "        (clone): Clone()\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (add): Add()\n",
       "      )\n",
       "      (clone): Clone()\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELU()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (add): Add()\n",
       "    )\n",
       "    (clone): Clone()\n",
       "  )\n",
       "  (7): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (matmul1): MatMul()\n",
       "        (matmul2): MatMul()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (add): Add()\n",
       "        (mul): Mul()\n",
       "        (clone): Clone()\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (add): Add()\n",
       "      )\n",
       "      (clone): Clone()\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELU()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (add): Add()\n",
       "    )\n",
       "    (clone): Clone()\n",
       "  )\n",
       "  (8): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (matmul1): MatMul()\n",
       "        (matmul2): MatMul()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (add): Add()\n",
       "        (mul): Mul()\n",
       "        (clone): Clone()\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (add): Add()\n",
       "      )\n",
       "      (clone): Clone()\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELU()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (add): Add()\n",
       "    )\n",
       "    (clone): Clone()\n",
       "  )\n",
       "  (9): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (matmul1): MatMul()\n",
       "        (matmul2): MatMul()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (add): Add()\n",
       "        (mul): Mul()\n",
       "        (clone): Clone()\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (add): Add()\n",
       "      )\n",
       "      (clone): Clone()\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELU()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (add): Add()\n",
       "    )\n",
       "    (clone): Clone()\n",
       "  )\n",
       "  (10): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (matmul1): MatMul()\n",
       "        (matmul2): MatMul()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (add): Add()\n",
       "        (mul): Mul()\n",
       "        (clone): Clone()\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (add): Add()\n",
       "      )\n",
       "      (clone): Clone()\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELU()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (add): Add()\n",
       "    )\n",
       "    (clone): Clone()\n",
       "  )\n",
       "  (11): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (matmul1): MatMul()\n",
       "        (matmul2): MatMul()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (add): Add()\n",
       "        (mul): Mul()\n",
       "        (clone): Clone()\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (add): Add()\n",
       "      )\n",
       "      (clone): Clone()\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELU()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (add): Add()\n",
       "    )\n",
       "    (clone): Clone()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.bert.children())\n",
    "# embeddings, encoder, pooler\n",
    "\n",
    "model.bert.encoder.layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import topic model\n",
    "classifier = model.classifier\n",
    "f_train = torch.from_numpy(np.load('models/imdb/bert/train_embeddings.npy'))\n",
    "n_concept = 10\n",
    "thres = 0.2\n",
    "\n",
    "# os.chdir('src/')\n",
    "# topic_model = topic_model_main(classifier, f_train, n_concept, thres, device, bert = True)\n",
    "graph_save_folder = 'models/imdb/bert/two_stage/'\n",
    "topic_model = torch.load(graph_save_folder + 'topic_model_two_stage.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualize class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thres:  0.3\n"
     ]
    }
   ],
   "source": [
    "#layers\n",
    "# topic_vector = topic_model.topic_vector.to(device)\n",
    "linearlayer1 = Linear(768, n_concept, bias = False)\n",
    "topic_vector_n = F.normalize(topic_model.topic_vector, dim = 0, p=2).transpose(1,0)\n",
    "linearlayer1.weight = nn.Parameter(topic_vector_n)\n",
    "\n",
    "thres = topic_model.thres\n",
    "print('thres: ', thres)\n",
    "\n",
    "linearlayer2 = Linear(n_concept, n_concept, bias = True)\n",
    "linearlayer2.weight = nn.Parameter(torch.eye(n_concept), requires_grad=False)\n",
    "linearlayer2.bias = nn.Parameter(torch.ones(n_concept)*(-thres), requires_grad = False)\n",
    "\n",
    "linearlayer3 = Linear(topic_model.rec_vector_1.shape[0], topic_model.rec_vector_1.shape[1], bias = False)\n",
    "linearlayer3.weight = nn.Parameter(topic_model.rec_vector_1.transpose(1,0))\n",
    "\n",
    "linearlayer4 = Linear(topic_model.rec_vector_2.shape[0], topic_model.rec_vector_2.shape[1], bias = False)\n",
    "linearlayer4.weight = nn.Parameter(topic_model.rec_vector_2.transpose(1,0))\n",
    "# new_classifier = Sequential(linearlayer1)\n",
    "# new classifier\n",
    "new_classifier = Sequential(NormLayer(),\n",
    "                           linearlayer1,\n",
    "                            linearlayer2,\n",
    "                            ReLU(),\n",
    "                           NormLayer(),\n",
    "                           linearlayer3,\n",
    "                            ReLU(),\n",
    "                           linearlayer4,\n",
    "                           classifier).to(device)\n",
    "model.classifier = new_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the explanations generator\n",
    "explanations = Generator(model)\n",
    "\n",
    "# classifications = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "classifications = range(n_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  tensor([[ 96.3010, -79.8617]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "[('[CLS]', 0.0), ('this', 0.7241052985191345), ('movie', 0.878639817237854), ('was', 0.8026872277259827), ('the', 0.33629634976387024), ('best', 0.41377565264701843), ('movie', 0.35376426577568054), ('i', 0.1672552227973938), ('have', 0.2121099829673767), ('ever', 0.08996672183275223), ('seen', 0.3126651346683502), ('!', 0.7803391218185425), ('some', 0.19109849631786346), ('scenes', 0.30642256140708923), ('were', 0.24444608390331268), ('ridiculous', 0.09867827594280243), (',', 0.028233131393790245), ('but', 0.215656116604805), ('acting', 0.7725077867507935), ('was', 1.0), ('great', 0.907504677772522), ('.', 0.1846427321434021), ('[SEP]', 0.20668818056583405)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (96.30)</b></text></td><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(120, 75%, 57%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> i                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ever                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seen                    </font></mark><mark style=\"background-color: hsl(120, 75%, 61%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> scenes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ridiculous                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(120, 75%, 62%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> acting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 55%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encode a sentence\n",
    "text_batch = [\"This movie was the best movie I have ever seen! some scenes were ridiculous, but acting was great.\"]\n",
    "encoding = tokenizer(text_batch, return_tensors='pt')\n",
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "# true class is positive - 1\n",
    "true_class = 1\n",
    "\n",
    "# generate an explanation for the input\n",
    "expl = explanations.generate_LRP(input_ids=input_ids, attention_mask=attention_mask, start_layer=0)[0]\n",
    "# print('1: ', expl)\n",
    "# normalize scores\n",
    "expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
    "\n",
    "# print('2: ', expl)\n",
    "\n",
    "# get the model classification\n",
    "output = model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "print('output: ', output)\n",
    "classification = output.argmax(dim=-1).item()\n",
    "# get class name\n",
    "class_name = classifications[classification]\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "print([(tokens[i], expl[i].item()) for i in range(len(tokens))])\n",
    "vis_data_records = [visualization.VisualizationDataRecord(\n",
    "                                expl,\n",
    "                                output[0][classification],\n",
    "                                classification,\n",
    "                                true_class,\n",
    "                                true_class,\n",
    "                                1,       \n",
    "                                tokens,\n",
    "                                1)]\n",
    "fig = visualization.visualize_text(vis_data_records)\n",
    "with open(\"data.html\", \"w\") as file:\n",
    "    file.write(fig.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut off classifier\n",
    "new_cut_classifier = Sequential(NormLayer(),\n",
    "                           linearlayer1,\n",
    "                            linearlayer2,\n",
    "                            ReLU(),\n",
    "                           NormLayer()).to(device)\n",
    "model.classifier = new_cut_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the explanations generator\n",
    "explanations = Generator(model)\n",
    "\n",
    "# classifications = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "classifications = range(n_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  tensor([[0.0000, 0.0000, 0.4932, 0.0000, 0.4920, 0.0000, 0.4431, 0.0000, 0.2685,\n",
      "         0.4961]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Classes with importance >0:  [2, 4, 6, 8, 9]\n",
      "classification:  9\n",
      "[('[CLS]', 0.0), ('this', 0.6721650958061218), ('movie', 0.4240603744983673), ('was', 0.5798816084861755), ('the', 0.4014686048030853), ('best', 0.8619788885116577), ('movie', 0.31506285071372986), ('i', 0.3781983554363251), ('have', 0.15347115695476532), ('ever', 0.08573821932077408), ('seen', 0.11877365410327911), ('!', 0.8603025674819946), ('some', 0.11571356654167175), ('scenes', 0.19586196541786194), ('were', 0.2765940725803375), ('ridiculous', 0.20337171852588654), (',', 0.14511419832706451), ('but', 0.25527259707450867), ('acting', 0.818137526512146), ('was', 1.0), ('great', 0.8157693147659302), ('.', 0.6944469213485718), ('[SEP]', 0.12099483609199524)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>9 (0.50)</b></text></td><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 57%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> i                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ever                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seen                    </font></mark><mark style=\"background-color: hsl(120, 75%, 57%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> scenes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ridiculous                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(120, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> acting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 66%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>9 (0.50)</b></text></td><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 57%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> i                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ever                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seen                    </font></mark><mark style=\"background-color: hsl(120, 75%, 57%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> scenes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ridiculous                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(120, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> acting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 66%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate an explanation for the input\n",
    "expl = explanations.generate_LRP(input_ids=input_ids, attention_mask=attention_mask, start_layer=0, index = 9)[0]\n",
    "# normalize scores\n",
    "expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
    "\n",
    "# print('2: ', expl)\n",
    "\n",
    "# get the model classification\n",
    "output = model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "print('output: ', output)\n",
    "indices = np.argwhere(output.cpu().detach().numpy()>0)\n",
    "print('Classes with importance >0: ', [i[1] for i in indices])\n",
    "classification = output.argmax(dim=-1).item()\n",
    "print('classification: ', classification)\n",
    "# get class name\n",
    "class_name = classifications[classification]\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "print([(tokens[i], expl[i].item()) for i in range(len(tokens))])\n",
    "vis_data_records = [visualization.VisualizationDataRecord(\n",
    "                                expl,\n",
    "                                output[0][classification],\n",
    "                                classification,\n",
    "                                true_class, #this changes the column 'true_label'\n",
    "                                true_class,\n",
    "                                1,       \n",
    "                                tokens,\n",
    "                                1)]\n",
    "visualization.visualize_text(vis_data_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO_k1BtSPVt3"
   },
   "source": [
    "**Negative sentiment example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "gD4xcvovI1KI",
    "outputId": "67842699-855a-4f22-de6d-6fd63d78f944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[CLS]', -0.0), ('i', -0.153622567653656), ('really', -0.16307400166988373), ('didn', -0.19001001119613647), (\"'\", -0.08315945416688919), ('t', -0.2679630219936371), ('like', -0.17981572449207306), ('this', -0.45753321051597595), ('movie', -0.1397773027420044), ('.', -0.22919468581676483), ('some', -0.08338094502687454), ('of', -0.03789211064577103), ('the', -0.05978512391448021), ('actors', -0.10391402989625931), ('were', -0.06579089164733887), ('good', -0.07998379319906235), (',', -0.06481628119945526), ('but', -0.3014720678329468), ('overall', -0.27188125252723694), ('the', -0.2926645576953888), ('movie', -0.13271962106227875), ('was', -0.6214057803153992), ('boring', -1.0), ('.', -0.31120413541793823), ('[SEP]', -0.06028567999601364)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.92)</b></text></td><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> i                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> really                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> didn                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> t                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> like                    </font></mark><mark style=\"background-color: hsl(0, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> actors                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> good                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> overall                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(0, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(0, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> boring                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.92)</b></text></td><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> i                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> really                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> didn                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> t                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> like                    </font></mark><mark style=\"background-color: hsl(0, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> actors                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> good                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> overall                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(0, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(0, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> boring                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode a sentence\n",
    "text_batch = [\"I really didn't like this movie. Some of the actors were good, but overall the movie was boring.\"]\n",
    "encoding = tokenizer(text_batch, return_tensors='pt')\n",
    "input_ids = encoding['input_ids'].to(\"cuda\")\n",
    "attention_mask = encoding['attention_mask'].to(\"cuda\")\n",
    "\n",
    "# true class is positive - 1\n",
    "true_class = 1\n",
    "\n",
    "# generate an explanation for the input\n",
    "expl = explanations.generate_LRP(input_ids=input_ids, attention_mask=attention_mask, start_layer=0)[0]\n",
    "# normalize scores\n",
    "expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
    "\n",
    "# get the model classification\n",
    "output = torch.nn.functional.softmax(model(input_ids=input_ids, attention_mask=attention_mask)[0], dim=-1)\n",
    "classification = output.argmax(dim=-1).item()\n",
    "# get class name\n",
    "class_name = classifications[classification]\n",
    "# if the classification is negative, higher explanation scores are more negative\n",
    "# flip for visualization\n",
    "if class_name == \"NEGATIVE\":\n",
    "  expl *= (-1)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "print([(tokens[i], expl[i].item()) for i in range(len(tokens))])\n",
    "vis_data_records = [visualization.VisualizationDataRecord(\n",
    "                                expl,\n",
    "                                output[0][classification],\n",
    "                                classification,\n",
    "                                true_class,\n",
    "                                true_class,\n",
    "                                1,       \n",
    "                                tokens,\n",
    "                                1)]\n",
    "visualization.visualize_text(vis_data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO4wn4/FB0J5pQCjUZNYulP",
   "include_colab_link": true,
   "name": "BERT-explainability.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('t5': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5a3f4ee66bfdb11b11cedd937192e1983f57eeabc863af72cdb14d87f4e47b4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
